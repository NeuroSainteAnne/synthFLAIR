{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MYbx9hT-w2BA"
   },
   "source": [
    "## Module loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4eoO16Yew2BG"
   },
   "outputs": [],
   "source": [
    "import h5py\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow.keras as keras\n",
    "import tensorflow as tf\n",
    "import os\n",
    "import nibabel as nib\n",
    "import random\n",
    "import re\n",
    "from sklearn.model_selection import train_test_split\n",
    "from natsort import natsorted\n",
    "from collections import Counter\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from tensorflow.keras import mixed_precision\n",
    "import progressbar\n",
    "from modules.generator import DataGenerator\n",
    "from modules.model import Generator, Discriminator\n",
    "from modules.losses import generator_loss, discriminator_loss\n",
    "from modules.figures import figure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define GPU Strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multiGPU = True\n",
    "mixedPrecision = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if multiGPU:\n",
    "    GPUstrategy = tf.distribute.MirroredStrategy()\n",
    "    def parallelize(func):\n",
    "        with GPUstrategy.scope():\n",
    "            func()\n",
    "else:\n",
    "    def parallelize(func):\n",
    "        func()\n",
    "        \n",
    "if mixedPrecision:\n",
    "    policy = mixed_precision.Policy('mixed_float16')\n",
    "    mixed_precision.set_global_policy(policy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kGZT5pbFw2BI"
   },
   "source": [
    "## Data loading\n",
    "\n",
    "The data must be in the following format :\n",
    "- one **metadata.hdf5** file containing the following variables :\n",
    "    - *\"patientnames\"*, a list with all patient identifiers\n",
    "    - *\"shape_x\"*, the numpy shape of the X array - typically, (n, 256, 256, 25, 3)\n",
    "    - *\"shape_y\"*, the numpy shape of the Y array - typically, (n, 256, 256, 25, 1)\n",
    "    - *\"shape_mask\"*, the numpy shape of the Brain mask array - typically, (n, 256, 256, 25, 1)\n",
    "    - *\"shape_meta\"*, the numpy shape of the Metadata array - typically, (n, 2)\n",
    "- Four **data_?.dat** files consisting in numpy memmaps\n",
    "    - *\"data_x.dat\"* in float32 with the following sequences stored in this order: \n",
    "        - b0 DWI (normalized with centered mean and divided by standard deviation)\n",
    "        - b1000 DWI (normalized with centered mean and divided by standard deviation)\n",
    "        - ADC computed in .10-6 mm2/sec\n",
    "    - *\"data_y.dat\"* in float32 with the realFLAIR sequences (normalized)\n",
    "    - *\"data_mask.dat\"* in uint8 with the brain weighting sequence\n",
    "        - value = 0 for out-of-brain voxels\n",
    "        - value = 1 for in-brain voxels\n",
    "        - value = 2 for stroke-region voxels\n",
    "    - *\"data_meta.dat\"* in float32 containing for each datapoint :\n",
    "        - the corresponding quality (0, 1, 2, 3) \n",
    "        - the corresponding timepoint (0, 1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5qRt0aQFw2BJ"
   },
   "outputs": [],
   "source": [
    "sourcedir = \"data/\" # Data directory\n",
    "\n",
    "with h5py.File(os.path.join(sourcedir,\"metadata.hdf5\"), \"r\") as data:\n",
    "    train_names = [l.decode() for l in list(data[\"patientnames\"])]\n",
    "    shape_x = tuple(data[\"shape_x\"])\n",
    "    shape_y = tuple(data[\"shape_y\"])\n",
    "    shape_mask = tuple(data[\"shape_mask\"])\n",
    "    shape_meta = tuple(data[\"shape_meta\"])\n",
    "    \n",
    "datax = np.memmap(os.path.join(sourcedir, \"data_x.dat\"), dtype=\"float32\", mode=\"r\", shape=shape_x)\n",
    "datay = np.memmap(os.path.join(sourcedir, \"data_y.dat\"), dtype=\"float32\", mode=\"r\", shape=shape_y)\n",
    "datamask = np.memmap(os.path.join(sourcedir, \"data_mask.dat\"), dtype=\"uint8\", mode=\"r\", shape=shape_mask)\n",
    "datameta = np.memmap(os.path.join(sourcedir, \"data_meta.dat\"), dtype=\"float32\", mode=\"r\", shape=shape_meta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "algnRIsjw2BK"
   },
   "source": [
    "## Stratified data splitting\n",
    "\n",
    "Data is split between train and test, with stratification on Quality and Datapoint\n",
    "Training data is then split betweeen train and validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9NicK4Z2w2BL"
   },
   "outputs": [],
   "source": [
    "TEST_SIZE = 0.2\n",
    "VALIDATION_SIZE = 0.2\n",
    "RANDOM_SEED = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7Klj9shRw2BM"
   },
   "outputs": [],
   "source": [
    "total_stratmri = []\n",
    "for i in range(shape_x[0]):\n",
    "    total_stratmri.append(str(int(datameta[i,0]))+\"_\"+str(int(datameta[i,1])))\n",
    "\n",
    "train_index, test_index = train_test_split(range(len(total_stratmri)), stratify=total_stratmri, \n",
    "                                           test_size=TEST_SIZE, random_state=RANDOM_SEED)\n",
    "\n",
    "print(\"Stratification count\")\n",
    "print(\"Training set: \", Counter([total_stratmri[i] for i in train_index]))\n",
    "print(\"Test set: \", Counter([total_stratmri[i] for i in test_index]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "j96f8AHvw2BM"
   },
   "outputs": [],
   "source": [
    "small_train_index, valid_index = train_test_split(train_index, test_size=VALIDATION_SIZE, \n",
    "                                                  shuffle=True, random_state=RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GTNxj64yw2BN"
   },
   "source": [
    "## Showing erratic data\n",
    "Looks up for volumes containing voxel values <50 or >50 and shows the middle slice.\n",
    "\n",
    "Please check the corresponding volumes of these patients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SHw5-D75w2BN"
   },
   "outputs": [],
   "source": [
    "flatmax = datay[...,0:2].max(axis=(1,2,3,4))\n",
    "flatmin = datay[...].min(axis=(1,2,3,4))\n",
    "erratic = np.where(np.logical_or(flatmax>50,flatmin<-50))[0]\n",
    "if len(erratic) > 0:\n",
    "    plt.rcParams['figure.figsize'] = [15, 5]\n",
    "    print([train_names[i] for i in erratic])\n",
    "    for i in range(len(erratic)):\n",
    "        j = erratic[i]\n",
    "        plt.subplot(1,len(erratic),i+1)\n",
    "        plt.imshow(np.flipud(datay[j,:,:,16,0].T), cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nE7GVSGhw2BO"
   },
   "source": [
    "## Checking data generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8e_OCGh-w2BQ"
   },
   "outputs": [],
   "source": [
    "check_generator = DataGenerator(datax=datax,\n",
    "                                datay=datay,\n",
    "                                datac=datameta.astype(np.uint8),\n",
    "                                mask=datamask,\n",
    "                                indices=np.arange(len(train_names)),\n",
    "                                shuffle=True, \n",
    "                                flatten_output=False,\n",
    "                                batch_size=1, dim_z=1,\n",
    "                                augment=True, flipaugm=True, brightaugm=[True,True,False], gpu_augment=True,\n",
    "                                scale_input=True, scale_input_lim=[(-5,12),(-5,12),(0,7500.0)], scale_input_clip=[True,True,False],\n",
    "                                scale_output=True, scale_output_lim=(-5,10), scale_output_clip=True,\n",
    "                                only_stroke=True, give_mask=True, give_meta=True)\n",
    "\n",
    "check_gen_iter = check_generator.getnext()\n",
    "\n",
    "plt.rcParams['figure.figsize'] = [15, 15]\n",
    "n_row = 4\n",
    "for i in range(n_row):\n",
    "    sampleX, sampleY = next(check_gen_iter)\n",
    "    plt.subplot(n_row,5,i*5+1)\n",
    "    plt.title('Diffusion imaging (b0)')\n",
    "    plt.imshow(np.flipud(sampleX[\"img\"][:,:,0,0].T), cmap='gray', vmin=-0.8, vmax=1)\n",
    "    plt.subplot(n_row,5,i*5+2)\n",
    "    plt.title('Diffusion imaging (b1000)')\n",
    "    plt.imshow(np.flipud(sampleX[\"img\"][:,:,0,1].T), cmap='gray', vmin=-0.8, vmax=1)\n",
    "    plt.subplot(n_row,5,i*5+3)\n",
    "    plt.title('ADC')\n",
    "    plt.imshow(np.flipud(sampleX[\"img\"][:,:,0,2].T), cmap='gray', vmin=-1.2, vmax=1)\n",
    "    plt.subplot(n_row,5,i*5+4)\n",
    "    plt.title('Mask')\n",
    "    plt.imshow(np.flipud(sampleX[\"mask\"][:,:,0].T), cmap='gray', vmin=0, vmax=2)\n",
    "    plt.subplot(n_row,5,i*5+5)\n",
    "    plt.title('real FLAIR')\n",
    "    plt.imshow(np.flipud(sampleY[:,:,0].T), cmap='gray', vmin=-0.8, vmax=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iwJW6Mm5w2BQ"
   },
   "source": [
    "## Create model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cs7iDbGnw2BQ"
   },
   "outputs": [],
   "source": [
    "LAMBDA_L1 = 100\n",
    "MAX_LAMBDA_EGDE = 100\n",
    "if mixedPrecision:\n",
    "    MASK_WEIGHTING = 3\n",
    "else:\n",
    "    MASK_WEIGHTING = 7 # in powers of ten\n",
    "\n",
    "figures_dir=\"figures\"\n",
    "log_dir=\"logs\"\n",
    "model_dir=\"models\"\n",
    "model_name=\"synthflair\"\n",
    "checkpoint_prefix = os.path.join(model_dir,model_name)\n",
    "\n",
    "epoch = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "p8LS-AFYw2BQ"
   },
   "outputs": [],
   "source": [
    "def defModel():\n",
    "    global generator, discriminator, generator_optimizer, discriminator_optimizer, checkpoint, summary_writer\n",
    "    generator = Generator()\n",
    "    discriminator = Discriminator()\n",
    "    generator_optimizer = tf.keras.optimizers.Adam(1e-5, beta_1=0.5)\n",
    "    discriminator_optimizer = tf.keras.optimizers.Adam(1e-5, beta_1=0.5)\n",
    "    checkpoint = tf.train.Checkpoint(generator=generator,\n",
    "                                     discriminator=discriminator,\n",
    "                                     generator_optimizer=generator_optimizer,\n",
    "                                     discriminator_optimizer=discriminator_optimizer)\n",
    "    summary_writer = tf.summary.create_file_writer(log_dir + \"/fit/\" + model_name)\n",
    "    if mixedPrecision:\n",
    "        generator_optimizer = mixed_precision.LossScaleOptimizer(generator_optimizer)\n",
    "        discriminator_optimizer = mixed_precision.LossScaleOptimizer(discriminator_optimizer)\n",
    "\n",
    "    return generator, discriminator, generator_optimizer, discriminator_optimizer, checkpoint, summary_writer\n",
    "\n",
    "parallelize(defModel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Auz7dL0Bw2BR"
   },
   "outputs": [],
   "source": [
    "# Load model if already trained \n",
    "def loadModel():\n",
    "    global epoch\n",
    "    if tf.train.latest_checkpoint(model_dir):\n",
    "        last_checkpoint = tf.train.latest_checkpoint(model_dir)\n",
    "        m = re.search('_epoch(\\d+)\\-', last_checkpoint)\n",
    "        last_epoch = m.group(1)\n",
    "        if last_epoch :\n",
    "            checkpoint.restore(last_checkpoint)\n",
    "            epoch = int(last_epoch) + 1  \n",
    "            \n",
    "parallelize(loadModel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tvVtls76w2BR"
   },
   "source": [
    "## Test model and figure export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "a4cjaPnmw2BS"
   },
   "outputs": [],
   "source": [
    "figure(datax, datay, datameta, datamask, generator, train_names, valid_index,\n",
    "       multiquality=False, slices=(3,5), save=False, show=True, n_patients=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rGav3fZFw2BS"
   },
   "source": [
    "## Define training and validation functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HdQyKZnKw2BS"
   },
   "outputs": [],
   "source": [
    "def train_step_(inputs, lambda_sobel, apply_gradients=True):\n",
    "    predictors, real_flair = inputs\n",
    "    quality = predictors[\"meta\"][...,0]\n",
    "    diffusion, mask = predictors[\"img\"], predictors[\"mask\"][:,:,:,0]\n",
    "    mask = tf.reshape(mask, mask.shape+(1,))\n",
    "\n",
    "    if mixedPrecision:\n",
    "        epsilon = 10e-7\n",
    "    else:\n",
    "        epsilon = 10e-12\n",
    "    weighted_mask = 10**((mask-1)*MASK_WEIGHTING)\n",
    "    mask_pooled = tf.keras.layers.MaxPool2D(32)(weighted_mask)\n",
    "\n",
    "    with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n",
    "        pseudo_flair = generator([diffusion, quality], training=True)\n",
    "        pseudo_filtered = tf.image.sobel_edges(pseudo_flair)\n",
    "        pseudo_sobel = tf.math.sqrt(tf.square(pseudo_filtered[...,0]) + tf.square(pseudo_filtered[...,1]) + epsilon)\n",
    "        real_filtered = tf.image.sobel_edges(real_flair)\n",
    "        real_sobel = tf.math.sqrt(tf.square(real_filtered[...,0]) + tf.square(real_filtered[...,1]) + epsilon)\n",
    "\n",
    "        disc_real_output = discriminator([diffusion, real_sobel, quality, real_flair], training=True)\n",
    "        disc_generated_output = discriminator([diffusion, pseudo_sobel, quality, pseudo_flair], training=True)\n",
    "        gen_total_loss, gen_gan_loss, gen_l1_loss, gen_edge_loss = generator_loss(disc_generated_output, \n",
    "                                                                               pseudo_flair, real_flair, LAMBDA_L1,\n",
    "                                                                               pseudo_sobel, real_sobel, lambda_sobel,\n",
    "                                                                               weighted_mask, mask_pooled)\n",
    "\n",
    "        disc_loss = discriminator_loss(disc_real_output, disc_generated_output, weighted_mask, mask_pooled)\n",
    "        \n",
    "        if mixedPrecision:\n",
    "            scaled_gen_loss = generator_optimizer.get_scaled_loss(gen_total_loss)\n",
    "            scaled_disc_loss = discriminator_optimizer.get_scaled_loss(disc_loss)\n",
    "\n",
    "        \n",
    "\n",
    "    if apply_gradients:\n",
    "        if mixedPrecision:\n",
    "            scaled_generator_gradients = gen_tape.gradient(scaled_gen_loss, generator.trainable_variables)\n",
    "            generator_gradients = generator_optimizer.get_unscaled_gradients(scaled_generator_gradients)\n",
    "            scaled_discriminator_gradients = disc_tape.gradient(scaled_disc_loss, discriminator.trainable_variables)\n",
    "            discriminator_gradients = discriminator_optimizer.get_unscaled_gradients(scaled_discriminator_gradients)\n",
    "        else:\n",
    "            generator_gradients = gen_tape.gradient(gen_total_loss,\n",
    "                                                    generator.trainable_variables)\n",
    "            discriminator_gradients = disc_tape.gradient(disc_loss,\n",
    "                                                       discriminator.trainable_variables)\n",
    "\n",
    "        generator_optimizer.apply_gradients(zip(generator_gradients,\n",
    "                                               generator.trainable_variables))\n",
    "        discriminator_optimizer.apply_gradients(zip(discriminator_gradients,\n",
    "                                                    discriminator.trainable_variables))\n",
    "\n",
    "    return {'gen_total_loss': gen_total_loss, 'gen_gan_loss': gen_gan_loss, \n",
    "            'gen_l1_loss': gen_l1_loss, 'gen_edge_loss': gen_edge_loss, 'disc_loss': disc_loss}\n",
    "\n",
    "def validation_step_(inputs, lambda_sobel):\n",
    "    return train_step_(inputs, lambda_sobel, False)\n",
    "\n",
    "if multiGPU:\n",
    "    with GPUstrategy.scope():\n",
    "        @tf.function\n",
    "        def train_step(dataset_inputs, epoch):\n",
    "            per_replica_losses = GPUstrategy.run(train_step_, args=(dataset_inputs,epoch))\n",
    "            all_replica_losses = {}\n",
    "            for l in per_replica_losses.keys():\n",
    "                all_replica_losses[l] = GPUstrategy.reduce(tf.distribute.ReduceOp.SUM, per_replica_losses[l], axis=(0,))/BATCH_SIZE\n",
    "            return all_replica_losses\n",
    "\n",
    "        @tf.function\n",
    "        def validation_step(dataset_inputs, epoch):\n",
    "            per_replica_losses = GPUstrategy.run(validation_step_, args=(dataset_inputs,epoch))\n",
    "            all_replica_losses = {}\n",
    "            for l in per_replica_losses.keys():\n",
    "                all_replica_losses[l] = GPUstrategy.reduce(tf.distribute.ReduceOp.SUM, per_replica_losses[l], axis=(0,))/BATCH_SIZE\n",
    "            return all_replica_losses\n",
    "else:\n",
    "    @tf.function\n",
    "    def train_step(inputs, lambda_sobel, apply_gradients=True):\n",
    "        return train_step_(inputs, lambda_sobel, apply_gradients=True)\n",
    "    \n",
    "    @tf.function\n",
    "    def validation_step(inputs, lambda_sobel):\n",
    "        return validation_step_(inputs, lambda_sobel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9WDaxVRsw2BT"
   },
   "source": [
    "## Train/Validation split and Generators definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wdhKzlrLw2BT"
   },
   "outputs": [],
   "source": [
    "valid_generator = DataGenerator(datax, datay, datameta.astype(np.uint8),\n",
    "                            mask=datamask, indices=valid_index, shuffle=True, \n",
    "                            flatten_output=False, batch_size=1, dim_z=1,\n",
    "                            augment=True, brightaugm=[False,False,False], shapeaugm=False, flipaugm=False, gpu_augment=False,\n",
    "                            scale_input=True, scale_input_lim=[(-5,12),(-5,12),(0,7500.0)], scale_input_clip=[False,False,False],\n",
    "                            scale_output=True, scale_output_lim=(-5,10), scale_output_clip=True,\n",
    "                            only_stroke=False, give_mask=True, give_meta=True)\n",
    "\n",
    "train_generator = DataGenerator(datax, datay, datameta.astype(np.uint8),\n",
    "                            mask=datamask, indices=small_train_index, shuffle=True, \n",
    "                            flatten_output=False, batch_size=1, dim_z=1,\n",
    "                            augment=True, shapeaugm=True, brightaugm=[True,True,False], flipaugm=True, gpu_augment=True, \n",
    "                            scale_input=True, scale_input_lim=[(-5,12),(-5,12),(0,7500.0)], scale_input_clip=[False,False,False],\n",
    "                            scale_output=True, scale_output_lim=(-5,10), scale_output_clip=True,\n",
    "                            only_stroke=False, give_mask=True, give_meta=True)\n",
    "\n",
    "\n",
    "dsT_pre = tf.data.Dataset.from_generator(train_generator.getnext, \n",
    "           ({\"img\":keras.backend.floatx(),\n",
    "             \"mask\":keras.backend.floatx(),\n",
    "             \"meta\":keras.backend.floatx()}, \n",
    "            keras.backend.floatx()), \n",
    "           ({\"img\":(256,256,1,3), \"mask\":(256,256,1), \"meta\":(1,2)}, (256,256,1)))\n",
    "dsV_pre = tf.data.Dataset.from_generator(valid_generator.getnext,  \n",
    "           ({\"img\":keras.backend.floatx(),\n",
    "             \"mask\":keras.backend.floatx(),\n",
    "             \"meta\":keras.backend.floatx()}, \n",
    "            keras.backend.floatx()), \n",
    "           ({\"img\":(256,256,1,3), \"mask\":(256,256,1), \"meta\":(1,2)}, (256,256,1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NlxWXx9rw2BT"
   },
   "source": [
    "## Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9DgdAsLZw2BT"
   },
   "outputs": [],
   "source": [
    "NB_EPOCHS = 5000\n",
    "BATCH_SIZE = 128 # Maximize the batch size for your GPU\n",
    "NB_SUBEPOCHS = len(train_generator)//BATCH_SIZE\n",
    "VALIDATION_STEPS = len(valid_generator)//BATCH_SIZE\n",
    "VALIDATION_EACH_EPOCH = 10\n",
    "\n",
    "saveFigures = True\n",
    "FIGURE_EACH_EPOCH = 10\n",
    "\n",
    "saveModels = True\n",
    "MODELSAVE_EACH_EPOCH = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EkT8uWuvw2BU"
   },
   "outputs": [],
   "source": [
    "if multiGPU:\n",
    "    with GPUstrategy.scope():\n",
    "        dsT = GPUstrategy.experimental_distribute_dataset(dsT_pre.batch(BATCH_SIZE).prefetch(256))\n",
    "        dsV = GPUstrategy.experimental_distribute_dataset(dsV_pre.batch(BATCH_SIZE).prefetch(256))\n",
    "else:\n",
    "    dsT = dsT_pre.batch(BATCH_SIZE).prefetch(256)\n",
    "    dsV = dsV_pre.batch(BATCH_SIZE).prefetch(256)\n",
    "\n",
    "def train_loop():\n",
    "    global epoch\n",
    "    for epoch in range(epoch, NB_EPOCHS+1):\n",
    "        print(\"Epoch\", epoch)\n",
    "        widgets = [\n",
    "            progressbar.Percentage(),\n",
    "            progressbar.Bar(),\n",
    "            \"    \",\n",
    "            progressbar.DynamicMessage('gen_total_loss', format=\"{name}: {formatted_value}\", precision=4),\n",
    "            \"    \",\n",
    "            progressbar.DynamicMessage('gen_gan_loss', format=\"{name}: {formatted_value}\", precision=4),\n",
    "            \"    \",\n",
    "            progressbar.DynamicMessage('gen_l1_loss', format=\"{name}: {formatted_value}\", precision=4),\n",
    "            \"    \",\n",
    "            progressbar.DynamicMessage('gen_edge_loss', format=\"{name}: {formatted_value}\", precision=4),\n",
    "            \"    \",\n",
    "            progressbar.DynamicMessage('disc_loss', format=\"{name}: {formatted_value}\", precision=4),\n",
    "            \"    \",\n",
    "            progressbar.ETA()\n",
    "        ]\n",
    "        progbar = progressbar.ProgressBar(max_value=NB_SUBEPOCHS, widgets=widgets, term_width=150)\n",
    "        subepoch = 0       \n",
    "        for dat in dsT:\n",
    "            if multiGPU:\n",
    "                if GPUstrategy.experimental_local_results(dat[0][\"img\"])[-1].shape[0] == 0:\n",
    "                    break\n",
    "            else:\n",
    "                if dat[0][\"img\"].shape[0] == 0:\n",
    "                    break\n",
    "            losses = train_step(dat, epoch if epoch < MAX_LAMBDA_EGDE else MAX_LAMBDA_EGDE)\n",
    "            step = epoch * NB_SUBEPOCHS + subepoch\n",
    "            with summary_writer.as_default():\n",
    "                tf.summary.scalar('gen_total_loss', losses[\"gen_total_loss\"].numpy().mean(), step=step)\n",
    "                tf.summary.scalar('gen_gan_loss', losses[\"gen_gan_loss\"].numpy().mean(), step=step)\n",
    "                tf.summary.scalar('gen_l1_loss', losses[\"gen_l1_loss\"].numpy().mean(), step=step)\n",
    "                tf.summary.scalar('gen_edge_loss', losses[\"gen_edge_loss\"].numpy().mean(), step=step)\n",
    "                tf.summary.scalar('disc_loss', losses[\"disc_loss\"].numpy().mean(), step=step)\n",
    "            progbar.update(subepoch, gen_total_loss=losses[\"gen_total_loss\"].numpy().mean(),\n",
    "                           gen_gan_loss=losses[\"gen_gan_loss\"].numpy().mean(),\n",
    "                           gen_l1_loss=losses[\"gen_l1_loss\"].numpy().mean(),\n",
    "                           gen_edge_loss=losses[\"gen_edge_loss\"].numpy().mean(),\n",
    "                           disc_loss=losses[\"disc_loss\"].numpy().mean())\n",
    "            subepoch += 1\n",
    "            if subepoch >= NB_SUBEPOCHS:\n",
    "                break\n",
    "\n",
    "        # Saving Trained Model\n",
    "        if saveModels and epoch % MODELSAVE_EACH_EPOCH == 0:\n",
    "            checkpoint.save(file_prefix = checkpoint_prefix+\"_epoch\"+str(epoch))\n",
    "\n",
    "        # Saving validation figures\n",
    "        if saveFigures and epoch % FIGURE_EACH_EPOCH == 0:\n",
    "            figure(datax, datay, datameta, datamask, generator, train_names, valid_index,\n",
    "                   multiquality=True, save=True, show=False, n_patients=10, epoch=epoch,\n",
    "                   output=figures_dir)\n",
    "\n",
    "        # Validation step\n",
    "        if epoch % VALIDATION_EACH_EPOCH == 0:\n",
    "            all_losses = []\n",
    "            mean_losses = []\n",
    "            print(\"VALIDATING at epoch\", epoch)\n",
    "            progbar = tf.keras.utils.Progbar(VALIDATION_STEPS)\n",
    "            subepoch = 0\n",
    "            for dat in dsV:\n",
    "                if multiGPU:\n",
    "                    if GPUstrategy.experimental_local_results(dat[0][\"img\"])[-1].shape[0] == 0:\n",
    "                        break\n",
    "                else:\n",
    "                    if dat[0][\"img\"].shape[0] == 0:\n",
    "                        break\n",
    "                all_losses.append(validation_step(dat, epoch))\n",
    "                progbar.update(subepoch, [(i,losses[i].numpy().mean()) for i in losses])  \n",
    "                subepoch += 1  \n",
    "                if subepoch >= VALIDATION_STEPS:\n",
    "                    break\n",
    "            mean_losses={key:np.mean([all_losses[i][key].numpy().mean() for i in range(len(all_losses))])\n",
    "                        for key in [\"gen_total_loss\",\"gen_gan_loss\",\"gen_l1_loss\",\"gen_edge_loss\",\"disc_loss\"]}\n",
    "            print(\"\")\n",
    "            with summary_writer.as_default():\n",
    "                tf.summary.scalar('valid_gen_total_loss', mean_losses[\"gen_total_loss\"], step=epoch*NB_SUBEPOCHS)\n",
    "                tf.summary.scalar('valid_gen_gan_loss', mean_losses[\"gen_gan_loss\"], step=epoch*NB_SUBEPOCHS)\n",
    "                tf.summary.scalar('valid_gen_l1_loss', mean_losses[\"gen_l1_loss\"], step=epoch*NB_SUBEPOCHS)\n",
    "                tf.summary.scalar('valid_gen_edge_loss', mean_losses[\"gen_edge_loss\"], step=epoch*NB_SUBEPOCHS)\n",
    "                tf.summary.scalar('valid_disc_loss', mean_losses[\"disc_loss\"], step=epoch*NB_SUBEPOCHS)\n",
    "    checkpoint.save(file_prefix = checkpoint_prefix+\"_epoch\"+str(epoch))\n",
    "    \n",
    "parallelize(train_loop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint.save(file_prefix = checkpoint_prefix+\"_epoch\"+str(epoch))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6vE497K8w2BU"
   },
   "source": [
    "## Test export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MkJZIpAAw2BU"
   },
   "outputs": [],
   "source": [
    "for i in test_index:\n",
    "    figure(datax, datay, datameta, datamask, generator, train_names, [i],\n",
    "           multiquality=False, save=True, show=False, n_patients=1, show_outline=True,\n",
    "          output=\"output\", savestr=train_names[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator.save('saved_generator')"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "synthFLAIR.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
